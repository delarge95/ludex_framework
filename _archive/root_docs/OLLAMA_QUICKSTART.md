# üöÄ Gu√≠a R√°pida: Usar Ollama con ARA Framework

## ‚úÖ Configuraci√≥n Completada

El sistema ya est√° completamente configurado con **Ollama (Mistral 7B)** como proveedor alternativo a GitHub Models.

### Verificaci√≥n (ya hecho):

- ‚úÖ Ollama server corriendo en `http://localhost:11434`
- ‚úÖ Modelo Mistral 7B descargado (4.4GB)
- ‚úÖ Paquetes instalados: `langchain-ollama v1.0.0`
- ‚úÖ Tool calling verificado: **4/4 tests pasados (100%)**
- ‚úÖ Integraci√≥n completa en `research_graph.py`

---

## üéØ Uso: Cambiar de Proveedor

### Opci√≥n 1: Variable de Entorno (Recomendado)

```powershell
# Usar Ollama (sin l√≠mites, desarrollo)
$env:USE_OLLAMA="true"
python main.py

# Volver a GitHub Models (calidad producci√≥n)
$env:USE_OLLAMA="false"
python main.py
```

### Opci√≥n 2: Modificar C√≥digo Directamente

```python
# En graphs/research_graph.py (l√≠nea ~68)
USE_OLLAMA = True   # Forzar Ollama
# o
USE_OLLAMA = False  # Forzar GitHub Models
```

---

## üß™ Scripts de Prueba Disponibles

### 1. Comparaci√≥n Completa (Agent 1)

```bash
python test_ollama_vs_github.py
```

**Ejecuta**: Agent 1 con ambos proveedores y compara:

- ‚è±Ô∏è Tiempo de ejecuci√≥n
- üìù Longitud de output
- üéØ Calidad de an√°lisis (viability score, trends, keywords)
- üîß Uso de herramientas

**Duraci√≥n**: ~10-15 minutos (5-8 min por proveedor)

### 2. Test Individual con Ollama

```bash
$env:USE_OLLAMA="true"
python test_single_agent.py
```

### 3. Pipeline Completo con Ollama

```bash
$env:USE_OLLAMA="true"
python main.py
```

‚ö†Ô∏è **Nota**: Agent 2 puede fallar con 40 papers (63K tokens > 32K l√≠mite de Mistral). Usar configuraci√≥n actual de 15 papers (v2.2c).

---

## üìä Comparaci√≥n R√°pida

| Aspecto        | GitHub Models | Ollama                 |
| -------------- | ------------- | ---------------------- |
| **Modelo**     | gpt-4o        | mistral:7b             |
| **Context**    | 128K tokens   | 32K tokens             |
| **Rate Limit** | 50 req/d√≠a ‚ö†Ô∏è | ‚àû Ilimitado ‚úÖ         |
| **Velocidad**  | M√°s r√°pido    | ~2x m√°s lento          |
| **Calidad**    | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê    | ‚≠ê‚≠ê‚≠ê‚≠ê (por validar) |
| **Costo**      | $0 (beta)     | $0 (local)             |
| **Uso**        | Producci√≥n    | Desarrollo             |

---

## üéØ Estrategias Recomendadas

### Estrategia 1: Desarrollo + Validaci√≥n Final

```bash
# Desarrollo e iteraci√≥n r√°pida (sin l√≠mites)
$env:USE_OLLAMA="true"
python main.py  # Ejecutar N veces sin preocupaci√≥n

# Validaci√≥n final antes de entregar
$env:USE_OLLAMA="false"
python main.py  # M√°xima calidad
```

### Estrategia 2: H√≠brida por Agente

```python
# En research_graph.py, modificar cada agente:
def niche_analyst_node(state):
    # Agent 1: Ollama (ligero, 15 papers)
    llm = create_model("ollama", "mistral:7b")

def literature_researcher_node(state):
    # Agent 2: GitHub (requiere 128K context para 40 papers)
    llm = create_model("github", "gpt-4o")

def technical_architect_node(state):
    # Agent 3: Ollama (no requiere contexto grande)
    llm = create_model("ollama", "mistral:7b")
```

---

## ‚ö†Ô∏è Limitaciones de Ollama

1. **Context Window**: 32K (vs 128K de gpt-4o)

   - ‚úÖ Agent 1 con 15 papers: ~19K tokens ‚Üí OK
   - ‚ùå Agent 2 con 40 papers: ~63K tokens ‚Üí Excede l√≠mite
   - **Soluci√≥n**: Mantener 15 papers o usar GitHub para Agent 2

2. **Velocidad**: ~2x m√°s lento que gpt-4o

   - Agent 1: 6-8 min vs 3-5 min
   - Aceptable para desarrollo

3. **Calidad**: Por validar en prueba completa
   - Tests unitarios: ‚úÖ 100% exitosos
   - Test E2E: ‚è≥ Pendiente (`test_ollama_vs_github.py`)

---

## üîß Troubleshooting

### Error: "Connection refused to localhost:11434"

```bash
# Verificar que Ollama est√© corriendo
# En la terminal donde funciona `ollama list`:
ollama serve

# O verificar v√≠a HTTP:
curl http://localhost:11434/api/tags
```

### Error: "Model mistral:7b not found"

```bash
# Descargar Mistral (4.4GB)
ollama pull mistral:7b

# Verificar descarga
ollama list
```

### Error: Context length exceeded

```bash
# Reducir n√∫mero de papers en Agent 1/2
# En graphs/research_graph.py buscar:
max_results=15  # Reducir si necesario
```

---

## üìÅ Archivos Relevantes

### Configuraci√≥n:

- `config/settings.py` - Variables `OLLAMA_*`
- `graphs/research_graph.py` - Integraci√≥n en 5 agentes

### Testing:

- `test_ollama_mistral.py` - Test unitario tool calling (‚úÖ 4/4)
- `test_ollama_vs_github.py` - Comparaci√≥n completa
- `check_ollama_setup.py` - Diagn√≥stico (‚úÖ 5/6 checks)

### Documentaci√≥n:

- `GUIA_OLLAMA.md` - Gu√≠a completa (450 l√≠neas)
- `OPTIMIZACIONES_MODELOS.md` - Secci√≥n v2.3
- `EVALUACION_MODELOS_OLLAMA.md` - An√°lisis 9 modelos

---

## üéâ ¬øQu√© Sigue?

### Pr√≥ximo paso recomendado:

```bash
# Ejecutar comparaci√≥n completa
python test_ollama_vs_github.py
```

Esto te dar√°:

- ‚úÖ Confirmaci√≥n de que Ollama funciona en escenario real
- üìä M√©tricas de calidad vs GitHub Models
- üí° Recomendaci√≥n de cu√°l estrategia usar

**Duraci√≥n**: ~15 minutos  
**Resultado**: Decisi√≥n informada sobre cu√°ndo usar cada proveedor

---

## üìû Soporte

Ver documentaci√≥n completa en:

- `GUIA_OLLAMA.md` - Setup detallado y troubleshooting
- `OPTIMIZACIONES_MODELOS.md` - Estrategias y arquitectura
- `core/model_factory.py` - C√≥digo fuente del factory pattern

**Estado actual**: ‚úÖ **SISTEMA LISTO PARA USAR**
