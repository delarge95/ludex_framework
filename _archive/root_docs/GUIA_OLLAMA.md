# üöÄ Gu√≠a R√°pida: Configuraci√≥n y Prueba de Ollama Mistral

## üìã Resumen de lo que hicimos

### ‚úÖ Configuraciones Completadas

1. **`config/settings.py`**: Agregado soporte para Ollama

   ```python
   OLLAMA_BASE_URL: str = "http://localhost:11434"
   OLLAMA_MODEL: str = "mistral:7b"
   OLLAMA_MODELS_PATH: str = r"E:\modelos_ollama"
   OLLAMA_TEMPERATURE: float = 0.7
   OLLAMA_NUM_CTX: int = 32768  # 32K context window
   ```

2. **`requirements.txt`**: Agregado `langchain-ollama>=0.2.0`

   - ‚úÖ Instalado correctamente

3. **`core/model_factory.py`**: Creado factory para modelos

   - `create_github_model()` - GitHub Models
   - `create_ollama_model()` - Ollama local
   - `create_model()` - Universal factory

4. **`test_ollama_mistral.py`**: Suite de pruebas completa
   - Test 0: Conexi√≥n b√°sica
   - Test 1: Tool calling simple
   - Test 2: M√∫ltiples herramientas
   - Test 3: Escenario realista

---

## üîß C√≥mo Ejecutar las Pruebas

### Opci√≥n 1: Desde la terminal donde ejecutaste `ollama pull`

**Esa terminal YA tiene Ollama configurado correctamente.**

```bash
# 1. Aseg√∫rate que Ollama server est√° corriendo
ollama serve

# 2. En OTRA terminal (desde ara_framework), ejecuta:
cd D:\Downloads\TRABAJO_DE_GRADO\ara_framework
python test_ollama_mistral.py
```

### Opci√≥n 2: Configurar PATH en PowerShell actual

```powershell
# Agregar Ollama al PATH (temporal, solo esta sesi√≥n)
$env:Path += ";C:\Users\alexw\AppData\Local\Programs\Ollama"  # Ajusta si es otra ubicaci√≥n

# Verificar
ollama list

# Debe mostrar: mistral:7b

# Luego ejecutar pruebas
python test_ollama_mistral.py
```

### Opci√≥n 3: Usar ruta completa de Ollama

Si sabes d√≥nde est√° instalado Ollama:

```powershell
# Ejemplo: C:\Program Files\Ollama\ollama.exe serve
# O: C:\Users\alexw\AppData\Local\Programs\Ollama\ollama.exe serve

# Buscar d√≥nde est√°:
Get-Command ollama -ErrorAction SilentlyContinue
# O buscar en Program Files:
Get-ChildItem -Path "C:\Program Files" -Filter "ollama.exe" -Recurse -ErrorAction SilentlyContinue
```

---

## üéØ Qu√© Esperar de las Pruebas

### ‚úÖ Si TODO funciona (mejor caso):

```
==================================================
 RESUMEN DE RESULTADOS
==================================================
‚úÖ PASADO - Test 0: Conexi√≥n b√°sica
‚úÖ PASADO - Test 1: Reconocimiento de herramientas
‚úÖ PASADO - Test 2: M√∫ltiples herramientas
‚úÖ PASADO - Test 3: Escenario realista

Total: 4/4 tests pasados (100.0%)
```

**Acci√≥n:** Mistral listo para desarrollo. Puedes usar Ollama sin l√≠mites.

### ‚ö†Ô∏è Si funciona PARCIALMENTE (caso probable):

```
==================================================
 RESUMEN DE RESULTADOS
==================================================
‚úÖ PASADO - Test 0: Conexi√≥n b√°sica
‚ùå FALLADO - Test 1: Reconocimiento de herramientas
‚ùå FALLADO - Test 2: M√∫ltiples herramientas
‚úÖ PASADO - Test 3: Escenario realista

Total: 2/4 tests pasados (50.0%)
```

**Raz√≥n:** Mistral puede responder en texto natural sin usar tool calling expl√≠citamente.

**Acci√≥n:** A√∫n √∫til para desarrollo b√°sico, pero herramientas pueden no funcionar igual que con GitHub Models.

### ‚ùå Si FALLA Test 0 (problema de conexi√≥n):

```
‚ùå ERROR en Test 0: Connection refused
```

**Raz√≥n:** Ollama server no est√° corriendo.

**Soluci√≥n:**

1. Abrir terminal donde funcion√≥ `ollama pull`
2. Ejecutar: `ollama serve`
3. Dejar corriendo en background
4. Volver a ejecutar: `python test_ollama_mistral.py`

---

## üìä Resultados Esperados de la Investigaci√≥n

Bas√°ndome en la documentaci√≥n oficial:

### ‚úÖ Mistral 7B v0.3 - Tool Calling Confirmado

**Documentaci√≥n oficial Ollama dice:**

> "Mistral 0.3 supports function calling with Ollama's raw mode"

**Formato esperado:**

```
[AVAILABLE_TOOLS] [{"type": "function", "function": {...}}][/AVAILABLE_TOOLS]
[INST] user prompt [/INST]
[TOOL_CALLS] [{"name": "tool_name", "arguments": {...}}]
```

**Compatibilidad LangChain:**

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(model="mistral:7b").bind_tools([my_tool])
# LangChain traduce autom√°ticamente al formato de Mistral
```

### ‚ö†Ô∏è Posibles Limitaciones

1. **Formato diferente a OpenAI**: Mistral usa formato especial con tags
2. **Precisi√≥n menor**: Puede no siempre usar tools cuando deber√≠a
3. **Context window**: 32K (vs 128K en gpt-4o)
4. **Calidad**: Probablemente menor que gpt-4o para an√°lisis complejos

---

## üîÑ Alternativas si Mistral No Funciona

### Plan B: Qwen2.5 8B

```bash
# En la terminal con Ollama configurado:
ollama pull qwen2.5:8b

# Luego ejecutar:
python test_ollama_mistral.py  # Modificar para usar qwen2.5:8b
```

**Ventajas:**

- Similar tama√±o (4.7GB vs 4.4GB)
- Mejor JSON generation (documentado)
- Tag "tools" presente en Ollama

**Desventajas:**

- Tool calling NO confirmado expl√≠citamente
- Necesita pruebas

### Plan C: Usar GitHub Models Solo Para Desarrollo Cr√≠tico

**Estrategia h√≠brida:**

1. **Pruebas locales r√°pidas**: Sin LLM, mock responses
2. **Pruebas de integraci√≥n**: GitHub Models (limitado a 50/d√≠a)
3. **Producci√≥n**: GitHub Models o alternativa cloud

---

## üí° Pr√≥ximos Pasos DESPU√âS de Ejecutar Pruebas

### Si Mistral funciona (Test 1-4 pasan):

#### 1. Modificar `test_single_agent.py` para usar Ollama

```python
# Agregar al inicio del archivo:
USE_OLLAMA = True  # Toggle para cambiar entre providers

# En la funci√≥n que crea el LLM:
if USE_OLLAMA:
    from core.model_factory import create_ollama_model
    llm = create_ollama_model(model="mistral:7b")
else:
    llm = ChatOpenAI(...)  # GitHub Models
```

#### 2. Comparar calidad de outputs

```bash
# GitHub Models
python test_single_agent.py  # Con USE_OLLAMA = False

# Ollama Mistral
python test_single_agent.py  # Con USE_OLLAMA = True

# Comparar:
# - Longitud de an√°lisis
# - Uso de herramientas (tool calls)
# - Coherencia y calidad
# - Tiempo de ejecuci√≥n
```

#### 3. Documentar en OPTIMIZACIONES_MODELOS.md

```markdown
## v2.3: Integraci√≥n Ollama para Desarrollo Local

**Fecha:** 2025-01-XX
**Objetivo:** Eliminar rate limits de GitHub Models durante desarrollo

**Configuraci√≥n:**

- Modelo: Mistral 7B v0.3
- Context: 32K tokens
- Tool calling: ‚úÖ Soportado
- Costo: $0.00 (local)
- Rate limit: ‚àû

**Resultados:**

- Test 1: [‚úÖ/‚ùå]
- Test 2: [‚úÖ/‚ùå]
- Test 3: [‚úÖ/‚ùå]
- Test 4: [‚úÖ/‚ùå]

**Decisi√≥n:**
[Usar Mistral para desarrollo / Mantener GitHub Models / Otro]
```

### Si Mistral falla parcialmente (solo Test 0-1 pasan):

#### Opci√≥n A: Aceptar limitaciones

- Usar Ollama para pruebas de flujo (sin herramientas)
- GitHub Models para pruebas de integraci√≥n completa
- Documentar limitaci√≥n conocida

#### Opci√≥n B: Probar Qwen2.5

```bash
ollama pull qwen2.5:8b
# Modificar test para usar qwen2.5
# Re-ejecutar pruebas
```

#### Opci√≥n C: Investigar configuraci√≥n avanzada

- Revisar logs de Ollama: `ollama logs`
- Verificar formato de tool calling espec√≠fico de Mistral
- Consultar documentaci√≥n: https://ollama.com/library/mistral

### Si Mistral falla completamente (Test 0 falla):

#### 1. Verificar instalaci√≥n de Ollama

```powershell
# Buscar proceso
Get-Process | Where-Object {$_.Name -like "*ollama*"}

# Buscar instalaci√≥n
Get-ChildItem -Path "C:\" -Filter "ollama.exe" -Recurse -ErrorAction SilentlyContinue
```

#### 2. Reinstalar si es necesario

- Descargar: https://ollama.com/download
- Instalar en ruta est√°ndar
- Configurar variable de entorno OLLAMA_MODELS=E:\modelos_ollama
- Reiniciar terminal

#### 3. Plan Alternativo

```markdown
**Decisi√≥n:** Mantener solo GitHub Models

**Mitigaci√≥n de rate limit:**

- Cache responses (Redis)
- Ejecutar tests menos frecuentemente
- Esperar reset diario (50 req/d√≠a es suficiente para 1-2 tests E2E)
- Considerar alternativas cloud:
  - Groq (14,400 req/d√≠a, gratis)
  - Gemini 2.5 Pro (1,500 req/d√≠a, gratis)
```

---

## üìù Checklist de Verificaci√≥n

Antes de reportar resultados:

- [ ] Ollama server est√° corriendo (`ollama serve` en terminal separada)
- [ ] Mistral descargado y visible (`ollama list` muestra mistral:7b)
- [ ] `langchain-ollama` instalado (`pip list | grep langchain-ollama`)
- [ ] Script ejecutado desde directorio correcto (`ara_framework/`)
- [ ] Puerto 11434 disponible (no bloqueado por firewall)

---

## üÜò Troubleshooting Com√∫n

### Error: "Connection refused"

**Causa:** Ollama server no est√° corriendo
**Soluci√≥n:** `ollama serve` en terminal separada

### Error: "Model not found"

**Causa:** Mistral no descargado o PATH incorrecta
**Soluci√≥n:** Verificar con `ollama list`, re-descargar si necesario

### Error: "Import error: langchain_ollama"

**Causa:** Paquete no instalado
**Soluci√≥n:** `pip install langchain-ollama`

### Warning: "No tool_calls detected"

**Causa:** Normal en algunos modelos, responden en texto
**Soluci√≥n:** No es error fatal, documentar comportamiento

---

## üìû Necesitas Ayuda?

Si encuentras problemas:

1. **Captura pantalla de error completo**
2. **Ejecuta diagn√≥stico:**
   ```bash
   ollama list
   ollama --version
   pip list | findstr langchain
   python --version
   ```
3. **Comparte resultados de test_ollama_mistral.py**
4. **Indica en qu√© paso fallaste**

---

**√öltima actualizaci√≥n:** 2025-01-XX  
**Estado:** Configuraci√≥n completada, pendiente ejecuci√≥n de pruebas  
**Pr√≥xima acci√≥n:** Ejecutar `python test_ollama_mistral.py` desde terminal con Ollama configurado
