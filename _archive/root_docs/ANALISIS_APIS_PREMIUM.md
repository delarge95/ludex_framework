# ğŸ” AnÃ¡lisis: Acceso a APIs Premium desde tus Suscripciones

**Fecha**: 12 Nov 2025  
**Objetivo**: Usar modelos incluidos en GitHub Copilot Pro, Perplexity Pro y Cursor

---

## ğŸ“Š Resumen Ejecutivo

| Plataforma             | Acceso API ProgramÃ¡tico       | Modelos Incluidos                                | Viabilidad           |
| ---------------------- | ----------------------------- | ------------------------------------------------ | -------------------- |
| **GitHub Copilot Pro** | âœ… **SÃ** (vÃ­a GitHub Models) | Claude Sonnet 4.5, GPT-4o, GPT-5, Gemini 2.5 Pro | â­â­â­â­â­ **IDEAL** |
| **Perplexity Pro**     | âœ… **SÃ** (API separada)      | Sonar Pro, Claude, GPT-4                         | â­â­â­â­ Buena       |
| **Cursor**             | âŒ **NO** (solo IDE)          | Claude, GPT-4, Gemini                            | â­ No viable         |

**RecomendaciÃ³n**: Usar **GitHub Copilot Pro + Perplexity API** juntos.

---

## 1ï¸âƒ£ GitHub Copilot Pro (TU MEJOR OPCIÃ“N)

### âœ… Acceso ProgramÃ¡tico: GitHub Models API

**DescripciÃ³n**: GitHub Copilot Pro incluye acceso a **GitHub Models**, que son los mismos modelos premium pero con API REST pÃºblica.

### ğŸ“‹ Modelos Disponibles (segÃºn tu screenshot):

```
âœ… GPT-4.1          - Ãšltimo modelo de OpenAI
âœ… GPT-4o           - Modelo multimodal de OpenAI
âœ… GPT-5 mini       - VersiÃ³n ligera de GPT-5
âœ… Claude Sonnet 4.5 - El mejor modelo de Anthropic
âœ… Claude Sonnet 4  - VersiÃ³n anterior
âœ… Claude Haiku 4.5 - Modelo rÃ¡pido de Anthropic
âœ… Gemini 2.5 Pro   - Modelo de Google (1M tokens contexto)
âœ… GPT-5            - Modelo mÃ¡s avanzado de OpenAI
âœ… GPT-5-Codex (Preview) - Especializado en cÃ³digo
âœ… Grok Code Fast 1 - Modelo de xAI
```

### ğŸ”‘ CÃ³mo Obtener el Token

#### OpciÃ³n A: Token Personal de GitHub (RECOMENDADO)

```bash
# 1. Ir a: https://github.com/settings/tokens
# 2. Click en "Generate new token (classic)"
# 3. Seleccionar scopes:
#    âœ… read:packages (REQUERIDO para GitHub Models)
#    âœ… read:user (opcional)
# 4. Copiar el token (empieza con ghp_)
# 5. Agregar a .env:
GITHUB_TOKEN=ghp_tu_token_aqui_xxxxxxxxxxxxxxxxxxxxxxxxxx
```

**LÃ­mites**:

- **50 requests por dÃ­a** por modelo (ya lo sabes por el error 429)
- Se resetea cada 24 horas
- **GRATIS** incluido en tu suscripciÃ³n de Copilot Pro ($10/mes)

#### OpciÃ³n B: Token OAuth de Copilot (MÃS LÃMITES)

Copilot usa OAuth internamente, pero tiene lÃ­mites mÃ¡s estrictos:

- ~300 requests premium/mes
- Requiere autenticaciÃ³n OAuth compleja
- No recomendado para uso programÃ¡tico

### ğŸ’» ImplementaciÃ³n en ARA Framework

**Ya estÃ¡ implementado** en tu cÃ³digo actual:

```python
# core/model_factory.py (lÃ­nea 40)
from langchain_openai import AzureChatOpenAI

llm = AzureChatOpenAI(
    api_key=settings.GITHUB_TOKEN,  # â† Tu token de GitHub
    azure_endpoint="https://models.inference.ai.azure.com",
    api_version="2024-05-01-preview",
    model="gpt-4o",  # o cualquier modelo de la lista
    temperature=0.7,
)
```

### ğŸ¯ Estrategia Recomendada con GitHub Models

```yaml
# DistribuciÃ³n inteligente para maximizar los 50 req/dÃ­a por modelo

Agent 1 (Niche Analyst):
  modelo: Claude Sonnet 4.5 # Mejor anÃ¡lisis cualitativo
  requests: ~1 por pipeline

Agent 2 (Literature Researcher):
  modelo: Gemini 2.5 Pro # 1M tokens contexto = cabe 40 papers
  requests: ~1 por pipeline
  fallback: GPT-4o (si Gemini falla)

Agent 3 (Technical Architect):
  modelo: GPT-5-Codex # Especializado en arquitectura
  requests: ~1 por pipeline

Agent 4 (Implementation Specialist):
  modelo: GPT-5-Codex # Especializado en cÃ³digo
  requests: ~1 por pipeline

Agent 5 (Content Synthesizer):
  modelo: GPT-5 # Mejor sÃ­ntesis y escritura
  requests: ~1 por pipeline

Total: 5 requests/pipeline Ã— 5 modelos = 25 requests/dÃ­a
Capacidad: 5 pipelines completos/dÃ­a sin repetir modelos
```

**Ventajas**:

- âœ… Cada agente usa el modelo mÃ¡s apropiado
- âœ… DistribuciÃ³n entre modelos evita rate limits
- âœ… 128K-1M contexto (suficiente para todos los agentes)
- âœ… Tool calling robusto en todos los modelos
- âœ… Calidad superior a Ollama Mistral 7B

---

## 2ï¸âƒ£ Perplexity Pro API

### âœ… Acceso ProgramÃ¡tico: Perplexity API

**DescripciÃ³n**: Perplexity Pro ($20/mes) **NO incluye crÃ©ditos API**. Debes pagar por la API por separado.

### ğŸ“‹ Modelos Disponibles

```
Perplexity Sonar Pro (128K contexto)
  - BÃºsqueda web en tiempo real
  - Citas automÃ¡ticas
  - Ideal para: Agent 1 (Niche Analyst)

Claude 3.5 Sonnet (200K contexto)
  - VÃ­a Perplexity API
  - MÃ¡s caro que directo

GPT-4 Turbo (128K contexto)
  - VÃ­a Perplexity API
```

### ğŸ’° Precios API (ADICIONAL a tu suscripciÃ³n Pro)

```
Sonar Pro:
  - $3.00 / 1M input tokens
  - $15.00 / 1M output tokens

Claude 3.5 Sonnet (vÃ­a Perplexity):
  - $3.00 / 1M input tokens
  - $15.00 / 1M output tokens
```

### ğŸ”‘ CÃ³mo Obtener API Key

```bash
# 1. Ir a: https://www.perplexity.ai/settings/api
# 2. Click "Create API Key"
# 3. Copiar el key (empieza con pplx-)
# 4. Agregar a .env:
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### ğŸ’» ImplementaciÃ³n

```python
# Agregar a core/model_factory.py
from openai import OpenAI

def create_perplexity_model(model: str = "sonar-pro", temperature: float = 0.7):
    """Create Perplexity API client (OpenAI-compatible)."""
    return OpenAI(
        api_key=settings.PERPLEXITY_API_KEY,
        base_url="https://api.perplexity.ai",
    ).chat.completions.create(
        model=model,
        temperature=temperature,
    )
```

### ğŸ¯ Caso de Uso Ideal

**Agent 1 (Niche Analyst)** con Sonar Pro:

- BÃºsqueda web en tiempo real
- AnÃ¡lisis de tendencias actuales
- Citas automÃ¡ticas de fuentes
- ~$0.50 por anÃ¡lisis completo

**Costo estimado**: ~$15/mes para 30 anÃ¡lisis (1 por dÃ­a)

### âš ï¸ LimitaciÃ³n

**Tu suscripciÃ³n Perplexity Pro ($20/mes)** te da:

- âœ… BÃºsquedas ilimitadas en la web UI
- âœ… Acceso a Claude, GPT-4 en UI
- âŒ **NO incluye crÃ©ditos API**

**Debes pagar API por separado** â†’ No es la mejor opciÃ³n econÃ³mica.

---

## 3ï¸âƒ£ Cursor (NO VIABLE)

### âŒ Sin Acceso API ProgramÃ¡tico

**DescripciÃ³n**: Cursor es un IDE (fork de VS Code) con IA integrada. **No expone API pÃºblica**.

### ğŸ“‹ Lo que incluye tu suscripciÃ³n

```
Cursor Pro ($20/mes):
  - Chat con Claude/GPT-4/Gemini en el IDE
  - Autocompletado de cÃ³digo
  - Agents para tareas de desarrollo

  âŒ NO tiene API REST
  âŒ NO se puede usar fuera del IDE
  âŒ NO compatible con LangChain/LangGraph
```

### ğŸ”§ Â¿Alternativa?

**Cursor Rules** (experimental):

- Puedes crear reglas personalizadas en `.cursorrules`
- Pero sigue siendo solo dentro del IDE
- No sirve para pipelines automatizados

### ğŸ¯ Mejor Uso de Cursor

1. **Desarrollo del framework** (editar cÃ³digo de ARA)
2. **Debugging interactivo** (usar Cursor Agent)
3. **DocumentaciÃ³n** (generar docs con IA)

**No usar para**: EjecuciÃ³n de pipelines de investigaciÃ³n.

---

## ğŸ“Š ComparaciÃ³n de Costos

### Escenario: 100 anÃ¡lisis/mes (pipeline completo)

| OpciÃ³n              | Modelos               | Costo/Mes                        | LÃ­mites           | Viabilidad           |
| ------------------- | --------------------- | -------------------------------- | ----------------- | -------------------- |
| **GitHub Models**   | Claude, GPT-5, Gemini | **$0** (incluido en Copilot Pro) | 50 req/dÃ­a/modelo | â­â­â­â­â­ **IDEAL** |
| **Perplexity API**  | Sonar Pro, Claude     | ~$50 (API adicional)             | Sin lÃ­mite        | â­â­â­ Caro          |
| **Ollama (actual)** | Mistral 7B            | $0 (local)                       | Sin lÃ­mite        | â­â­â­ Calidad baja  |
| **Cursor**          | N/A                   | N/A                              | No disponible     | âŒ No viable         |

### HÃ­brido Ã“ptimo (RECOMENDACIÃ“N FINAL)

```yaml
Costo Total: $10/mes (solo Copilot Pro que ya tienes)

DistribuciÃ³n:
  80% de requests: GitHub Models (Claude, GPT-5, Gemini)
    - Agent 1-5: 5 modelos diferentes por pipeline
    - Capacidad: ~10 pipelines/dÃ­a = 300/mes

  20% de requests: Ollama Mistral 7B (fallback local)
    - Cuando se agoten los 50 req/dÃ­a de algÃºn modelo
    - Para testing y desarrollo iterativo

  0% de requests: Perplexity API
    - No vale la pena el costo adicional
    - GitHub Models es gratis y mejor

Resultado:
  âœ… 300 pipelines completos/mes
  âœ… Calidad superior (Claude Sonnet, GPT-5)
  âœ… Sin costo adicional
  âœ… Fallback ilimitado con Ollama
```

---

## ğŸš€ Plan de ImplementaciÃ³n

### Fase 1: Configurar GitHub Models (15 min)

```bash
# 1. Obtener token
# Ir a: https://github.com/settings/tokens
# Generate new token (classic)
# Scope: read:packages âœ…
# Copiar token (ghp_xxx...)

# 2. Agregar a .env
echo "GITHUB_TOKEN=ghp_tu_token_aqui" >> ara_framework/.env

# 3. Verificar
cd ara_framework
python test_github_models_env.py
```

### Fase 2: Modificar model_factory.py (30 min)

```python
# core/model_factory.py

def create_model_smart(
    agent_name: str,
    temperature: float = 0.7,
) -> BaseChatModel:
    """
    SelecciÃ³n inteligente de modelo por agente.
    Maximiza uso de GitHub Models gratis.
    """

    # Mapa agente â†’ modelo Ã³ptimo
    MODEL_MAP = {
        "niche_analyst": {
            "provider": "github",
            "model": "Claude-3.5-Sonnet",  # Mejor anÃ¡lisis cualitativo
        },
        "literature_researcher": {
            "provider": "github",
            "model": "Gemini-2.5-Pro",  # 1M contexto para 40 papers
        },
        "technical_architect": {
            "provider": "github",
            "model": "gpt-5-codex-preview",  # Especialista cÃ³digo
        },
        "implementation_specialist": {
            "provider": "github",
            "model": "gpt-5-codex-preview",  # Especialista cÃ³digo
        },
        "content_synthesizer": {
            "provider": "github",
            "model": "gpt-5",  # Mejor escritura
        },
    }

    config = MODEL_MAP.get(agent_name)

    if not config:
        # Fallback a Ollama
        logger.warning(f"Agent {agent_name} no configurado, usando Ollama")
        return create_ollama_model(temperature=temperature)

    try:
        # Intentar GitHub Models primero
        return create_github_model(
            model=config["model"],
            temperature=temperature,
        )
    except Exception as e:
        # Si falla (rate limit), usar Ollama
        logger.warning(
            f"GitHub Models fallÃ³ para {agent_name}: {e}. Usando Ollama."
        )
        return create_ollama_model(temperature=temperature)
```

### Fase 3: Actualizar research_graph.py (15 min)

```python
# graphs/research_graph.py (modificar cada agente)

def niche_analyst_node(state):
    llm = create_model_smart(
        agent_name="niche_analyst",  # â† SelecciÃ³n automÃ¡tica
        temperature=0.7,
    )
    # resto del cÃ³digo igual...

def literature_researcher_node(state):
    llm = create_model_smart(
        agent_name="literature_researcher",
        temperature=0.7,
    )
    # resto del cÃ³digo igual...

# Repetir para Agent 3, 4, 5...
```

### Fase 4: Testing (20 min)

```bash
# Test 1: Verificar GitHub Models
python test_github_models_env.py

# Test 2: Pipeline completo con GitHub Models
$env:USE_GITHUB_MODELS="true"
python test_pipeline.py

# Test 3: ComparaciÃ³n GitHub vs Ollama
python test_github_vs_ollama.py  # Crear este script
```

### Fase 5: Monitoreo de Rate Limits (10 min)

```python
# Crear ara_framework/monitor_github_limits.py

import os
import requests
from datetime import datetime

GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

def check_rate_limits():
    """Verifica lÃ­mites restantes de GitHub Models."""

    headers = {"Authorization": f"Bearer {GITHUB_TOKEN}"}

    # GitHub Models usa el mismo rate limit que GitHub API
    response = requests.get(
        "https://api.github.com/rate_limit",
        headers=headers,
    )

    data = response.json()

    print("\n" + "="*60)
    print("ğŸ“Š GITHUB MODELS - LÃMITES RESTANTES")
    print("="*60)

    core = data["resources"]["core"]
    print(f"\nâœ… Requests disponibles: {core['remaining']}/{core['limit']}")
    print(f"â° Reset en: {datetime.fromtimestamp(core['reset'])}")

    # Calcular requests por modelo (estimado)
    models_count = 9  # NÃºmero de modelos en tu screenshot
    per_model = 50  # LÃ­mite por modelo/dÃ­a

    print(f"\nğŸ“Š Estimado por modelo:")
    print(f"   â€¢ LÃ­mite por modelo: {per_model} req/dÃ­a")
    print(f"   â€¢ Modelos disponibles: {models_count}")
    print(f"   â€¢ Capacity total: {per_model * models_count} req/dÃ­a")

    print("\n" + "="*60)

if __name__ == "__main__":
    check_rate_limits()
```

---

## ğŸ¯ PrÃ³ximos Pasos (ACCIÃ“N INMEDIATA)

### 1. Configurar GitHub Token (HOY - 5 min)

```bash
# Windows PowerShell
cd D:\Downloads\TRABAJO_DE_GRADO\ara_framework

# Ir a: https://github.com/settings/tokens
# Generate new token (classic)
# Scope: read:packages âœ…
# Copiar token

# Agregar a .env
echo "GITHUB_TOKEN=ghp_tu_token_aqui" >> .env

# Verificar
python test_github_models_env.py
```

### 2. Esperar a maÃ±ana para test completo (13 Nov)

**RazÃ³n**: Tu lÃ­mite actual se resetea en ~19 horas.

**Entonces ejecutar**:

```bash
# MaÃ±ana 13 Nov, ~11:00 AM
$env:USE_GITHUB_MODELS="true"
python test_pipeline.py
```

### 3. Implementar selecciÃ³n inteligente (MaÃ±ana - 1 hora)

Modificar `model_factory.py` y `research_graph.py` segÃºn Fase 2-3 arriba.

---

## ğŸ“ ConclusiÃ³n

### âœ… Respuesta a tu pregunta:

**"Â¿Puedo usar las IAs incluidas en Copilot Pro, Perplexity Pro y Cursor?"**

- âœ… **GitHub Copilot Pro**: SÃ, vÃ­a GitHub Models API (ya configurado en tu cÃ³digo)
- âš ï¸ **Perplexity Pro**: SÃ, pero requiere pago adicional de API (~$50/mes extra)
- âŒ **Cursor**: NO, es solo IDE sin API programÃ¡tica

### ğŸ† RecomendaciÃ³n Final:

**Usar exclusivamente GitHub Copilot Pro** con la estrategia de distribuciÃ³n de modelos:

```
Pipeline â†’ 5 agentes â†’ 5 modelos diferentes â†’ 1 request c/u
= 5 requests/pipeline
= 10 pipelines/dÃ­a posibles (50 req/dÃ­a por modelo)
= 300 pipelines/mes
= $0 adicional (incluido en tu $10/mes de Copilot Pro)
```

**Resultado**: Calidad superior a Ollama, sin costo adicional, y capacidad suficiente para tu trabajo de grado.

---

**Generado**: 12 Nov 2025  
**Siguiente paso**: Configurar `GITHUB_TOKEN` y esperar a maÃ±ana para ejecutar pipeline completo.
