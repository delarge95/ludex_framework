"""
Configuraci√≥n global del ARA Framework.

Fuente: docs/02_PROJECT_CONSTITUTION.md (Stack definitivo Nov 2025)
"""
import os
from typing import Literal, Optional
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """Configuraci√≥n del proyecto con validaci√≥n Pydantic."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore"
    )
    
    # ============================================================
    # ENVIRONMENT
    # ============================================================
    ENV: Literal["development", "production"] = "development"
    DEBUG: bool = True
    LOG_LEVEL: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = "INFO"
    LOG_FORMAT: Literal["json", "text"] = "json"
    
    # ============================================================
    # GITHUB COPILOT PRO + CONTINUE.DEV
    # ============================================================
    # Continue.dev maneja la autenticaci√≥n autom√°ticamente
    # No requiere API key expl√≠cita, usa GitHub OAuth
    
    # ============================================================
    # GEMINI 2.5 PRO (FREE - 1500 req/d√≠a)
    # ============================================================
    GEMINI_API_KEY: Optional[str] = None
    GEMINI_MODEL: str = "gemini-2.5-pro"
    GEMINI_TEMPERATURE: float = 0.7
    
    # ============================================================
    # DEEPSEEK V3 (685B MoE - $0.27/M input, $1.10/M output)
    # ============================================================
    DEEPSEEK_API_KEY: Optional[str] = None
    DEEPSEEK_MODEL: str = "deepseek-chat"
    DEEPSEEK_BASE_URL: str = "https://api.deepseek.com"
    DEEPSEEK_TEMPERATURE: float = 0.7
    
    # ============================================================
    # MINIMAX-M2 (229B MoE - Beta Gratuita)
    # ============================================================
    MINIMAX_API_KEY: Optional[str] = None
    MINIMAX_MODEL: str = "minimax-m2"
    MINIMAX_BASE_URL: str = "https://api.minimaxi.com/v1"
    MINIMAX_TEMPERATURE: float = 0.7
    
    # ============================================================
    # ANTHROPIC CLAUDE (via Copilot Pro - 0.33x/1x credits)
    # ============================================================
    ANTHROPIC_API_KEY: Optional[str] = None  # Opcional, Copilot Pro lo maneja
    
    # ============================================================
    # OPENAI (GPT-5, GPT-4o via Copilot Pro - 0x/1x credits)
    # ============================================================
    OPENAI_API_KEY: Optional[str] = None  # Opcional, Copilot Pro lo maneja
    
    # ============================================================
    # GROQ (LLaMA 3.3-70B - GRATIS - 14,400 req/d√≠a)
    # ============================================================
    GROQ_API_KEY: Optional[str] = None
    GROQ_MODEL: str = "llama-3.1-8b-instant"  # Modelo m√°s peque√±o para evitar rate limit
    GROQ_BASE_URL: str = "https://api.groq.com/openai/v1"
    GROQ_TEMPERATURE: float = 0.7
    
    # ============================================================
    # DATA INSPECTOR API KEY (Sprint 9)
    # ============================================================
    DATA_INSPECTOR_API_KEY: Optional[str] = None  # For /data/* endpoints
    
    # ============================================================
    # PERPLEXITY AI (Real-time web search + LLM)
    # ============================================================
    PERPLEXITY_API_KEY: Optional[str] = None
    PERPLEXITY_MODEL: str = "llama-3.1-sonar-large-128k-online"  # Best quality
    # Available models:
    # - llama-3.1-sonar-small-128k-online: Fast, cheaper
    # - llama-3.1-sonar-large-128k-online: Better quality (recommended)
    # - llama-3.1-sonar-huge-128k-online: Best quality, slower
    PERPLEXITY_BASE_URL: str = "https://api.perplexity.ai"
    
    # ============================================================
    # GITHUB MODELS (Beta - GRATIS durante beta)
    # ============================================================
    GITHUB_TOKEN: Optional[str] = None  # Personal Access Token with read:packages
    GITHUB_MODEL: str = "gpt-4o"  # Modelo por defecto
    # Available models (verificados Nov 2025):
    # OpenAI:
    # - gpt-4o: Mejor para an√°lisis, arquitectura, s√≠ntesis (RECOMENDADO)
    # - gpt-4o-mini: M√°s r√°pido y econ√≥mico
    # Meta Llama:
    # - Llama-3.3-70B-Instruct: √öltimo Llama 3.3 (NUEVO)
    # - Meta-Llama-3.1-405B-Instruct: Modelo m√°s grande (405B par√°metros)
    # - Meta-Llama-3.1-8B-Instruct: M√°s r√°pido
    # Microsoft Phi:
    # - Phi-4: √öltimo modelo Phi (NUEVO)
    # Mistral AI:
    # - Mistral-Nemo: Balance calidad/velocidad
    # - Mistral-small: M√°s r√°pido
    # Cohere:
    # - cohere-command-r-08-2024: Para tareas generales
    # - cohere-command-r-plus-08-2024: Versi√≥n mejorada
    # AI21 Labs:
    # - jamba-1.5-large: Modelo h√≠brido SSM-Transformer
    # Otras:
    # - ministral-3b: Modelo peque√±o (3B par√°metros)
    GITHUB_MODELS_BASE_URL: str = "https://models.inference.ai.azure.com"
    
    # ============================================================
    # MCP SERVERS - Tokens y URLs
    # ============================================================
    # GitHub MCP
    GITHUB_PERSONAL_ACCESS_TOKEN: Optional[str] = None
    
    # Jina AI Reader MCP (scraping avanzado)
    JINA_API_KEY: Optional[str] = None
    
    # Supabase MCP (PostgreSQL + Storage)
    SUPABASE_URL: Optional[str] = None
    SUPABASE_ANON_KEY: Optional[str] = None
    SUPABASE_SERVICE_ROLE_KEY: Optional[str] = None
    
    # Notion MCP (opcional)
    NOTION_API_KEY: Optional[str] = None
    
    # Composio/Rube MCP (workflows)
    COMPOSIO_API_KEY: Optional[str] = None
    
    # ============================================================
    # OLLAMA - Local LLM Server (TESTING/DEVELOPMENT)
    # ============================================================
    OLLAMA_BASE_URL: str = "http://localhost:11434"  # Ollama API endpoint
    OLLAMA_MODEL: str = "mistral:7b"  # Modelo por defecto para testing
    OLLAMA_MODELS_PATH: str = r"E:\modelos_ollama"  # Directorio de modelos
    OLLAMA_TEMPERATURE: float = 0.7
    OLLAMA_NUM_CTX: int = 32768  # Context window (32K para Mistral 7B)
    # Available models (verificar con: ollama list):
    # - mistral:7b: Tool calling support confirmado (4.4GB)
    # - qwen2.5:8b: Por verificar tool calling (4.7GB)
    # Uso recomendado: Development/testing (sin rate limits)
    
    # ============================================================
    # SEMANTIC SCHOLAR API (No requiere key, 1 req/seg limit)
    # ============================================================
    SEMANTIC_SCHOLAR_DELAY: float = 1.0  # 1 segundo entre requests (CR√çTICO)
    SEMANTIC_SCHOLAR_BASE_URL: str = "https://api.semanticscholar.org/graph/v1"
    
    # ============================================================
    # VALKEY/REDIS - Cache Configuration
    # ============================================================
    REDIS_URL: str = "redis://localhost:6379/0"
    REDIS_TTL_PAPERS: int = 604800  # 7 d√≠as (papers son estables)
    REDIS_TTL_CONTENT: int = 259200  # 3 d√≠as (scraped content)
    REDIS_TTL_ANALYSIS: int = 2592000  # 30 d√≠as (resultados de an√°lisis)
    REDIS_MAX_CONNECTIONS: int = 10
    
    # ============================================================
    # UPTRACE - Observability (1TB/mes gratis)
    # ============================================================
    UPTRACE_DSN: Optional[str] = None
    UPTRACE_SERVICE_NAME: str = "ara-framework"
    UPTRACE_SERVICE_VERSION: str = "0.1.0"
    
    # ============================================================
    # AGENT TIMEOUTS (segundos) - SLAs por agente
    # ============================================================
    AGENT_TIMEOUT_NICHE: int = 480  # 8 min (NicheAnalyst)
    AGENT_TIMEOUT_LITERATURE: int = 1500  # 25 min (LiteratureResearcher - bottleneck)
    AGENT_TIMEOUT_ARCHITECT: int = 720  # 12 min (TechnicalArchitect)
    AGENT_TIMEOUT_IMPLEMENTATION: int = 480  # 8 min (ImplementationSpecialist)
    AGENT_TIMEOUT_SYNTHESIS: int = 600  # 10 min (ContentSynthesizer)
    AGENT_TIMEOUT_ORCHESTRATOR: int = 420  # 7 min (Orchestrator)
    
    # ============================================================
    # BUDGET MANAGER - Copilot Pro Credits (300/mes)
    # ============================================================
    BUDGET_MAX_CREDITS_PER_MONTH: int = 300  # Copilot Pro limit
    BUDGET_ALERT_THRESHOLD: float = 0.80  # Alert at 80% usage (240 credits)
    BUDGET_PROJECTED_USAGE_PER_ANALYSIS: float = 0.45  # cr√©ditos por an√°lisis
    
    # Credit costs por modelo (seg√∫n docs/03_PROJECT_SPEC.md)
    CREDIT_COST_GPT5: float = 1.0  # GPT-5
    CREDIT_COST_CLAUDE_SONNET: float = 1.0  # Claude Sonnet 4.5
    CREDIT_COST_CLAUDE_HAIKU: float = 0.33  # Claude Haiku 4.5
    CREDIT_COST_GPT4O: float = 0.0  # GPT-4o (gratis ilimitado)
    CREDIT_COST_GEMINI: float = 0.0  # Gemini 2.5 Pro (gratis)
    CREDIT_COST_DEEPSEEK: float = 0.0  # DeepSeek V3 (pago directo, no cr√©ditos)
    CREDIT_COST_MINIMAX: float = 0.0  # MiniMax-M2 (beta gratis)
    
    # ============================================================
    # CIRCUIT BREAKER - Resilience Configuration
    # ============================================================
    CIRCUIT_BREAKER_FAIL_MAX: int = 5  # Max failures before opening
    CIRCUIT_BREAKER_TIMEOUT: int = 60  # Seconds to wait before half-open
    
    # ============================================================
    # RETRY STRATEGY - Tenacity Configuration
    # ============================================================
    RETRY_MAX_ATTEMPTS: int = 3
    RETRY_WAIT_EXPONENTIAL_MULTIPLIER: int = 1  # 1, 2, 4, 8... seconds
    RETRY_WAIT_EXPONENTIAL_MAX: int = 10  # Max wait time
    
    # ============================================================
    # BLENDER CONTROL (opcional para visualizaci√≥n 3D)
    # ============================================================
    BLENDER_PATH: Optional[str] = None
    BLENDER_ZMQ_PORT: int = 5555
    
    # ============================================================
    # PIPELINE CONFIGURATION
    # ============================================================
    PIPELINE_MAX_CONCURRENT_AGENTS: int = 3  # Paralelizaci√≥n limitada
    PIPELINE_ENABLE_CACHE: bool = True
    PIPELINE_ENABLE_TELEMETRY: bool = True
    
    # ============================================================
    # DATABASE - Supabase Tables
    # ============================================================
    DB_TABLE_ANALYSIS: str = "analysis_results"
    DB_TABLE_PAPERS: str = "papers_cache"
    DB_TABLE_BUDGET: str = "budget_tracking"
    
    # ============================================================
    # OUTPUT PATHS
    # ============================================================
    OUTPUT_DIR: str = "outputs"
    OUTPUT_FORMAT: Literal["markdown", "pdf", "html"] = "markdown"
    
    @property
    def is_production(self) -> bool:
        """Check if running in production."""
        return self.ENV == "production"
    
    @property
    def redis_client_kwargs(self) -> dict:
        """Redis client configuration."""
        return {
            "decode_responses": True,
            "max_connections": self.REDIS_MAX_CONNECTIONS,
            "socket_connect_timeout": 5,
            "socket_keepalive": True,
        }
    
    @property
    def uptrace_config(self) -> dict:
        """Uptrace configuration for OpenTelemetry."""
        return {
            "dsn": self.UPTRACE_DSN,
            "service_name": self.UPTRACE_SERVICE_NAME,
            "service_version": self.UPTRACE_SERVICE_VERSION,
            "deployment_environment": self.ENV,
        }


# Singleton instance
settings = Settings()


# Validaci√≥n al importar
def validate_critical_settings() -> None:
    """Valida que las configuraciones cr√≠ticas est√©n presentes."""
    critical_keys = [
        "GEMINI_API_KEY",
        "DEEPSEEK_API_KEY",
        "MINIMAX_API_KEY",
        "GITHUB_PERSONAL_ACCESS_TOKEN",
        "JINA_API_KEY",
        "SUPABASE_URL",
        "SUPABASE_ANON_KEY",
        "SUPABASE_SERVICE_ROLE_KEY",
        "UPTRACE_DSN",
    ]
    
    missing = []
    for key in critical_keys:
        value = getattr(settings, key, None)
        if not value:
            missing.append(key)
    
    if missing:
        raise ValueError(
            f"‚ùå Missing critical settings: {', '.join(missing)}\n"
            f"üìñ Ver docs/08_GETTING_STARTED.md para obtener las API keys"
        )


# Ejecutar validaci√≥n al importar (solo en producci√≥n)
if settings.ENV == "production":
    validate_critical_settings()
