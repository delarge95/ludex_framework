# üöÄ GU√çA DE IMPLEMENTACI√ìN - Stack de Modelos IA (ARA Framework)

**Versi√≥n**: 1.0 Final  
**Fecha**: Noviembre 2025  
**Presupuesto Recomendado**: $10-18/mes  
**Costo Real Proyectado**: $10-15/mes (para 100+ an√°lisis/mes)

---

## üìã Tabla de Contenidos

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Fase 1: Setup Inicial (D√≠a 1)](#fase-1-setup-inicial-d√≠a-1)
3. [Fase 2: Validaci√≥n y Testing (D√≠a 2-3)](#fase-2-validaci√≥n-y-testing-d√≠a-2-3)
4. [Fase 3: Deployment en Producci√≥n (Semana 1)](#fase-3-deployment-en-producci√≥n-semana-1)
5. [Fase 4: Monitoreo y Optimizaci√≥n (Continuo)](#fase-4-monitoreo-y-optimizaci√≥n-continuo)
6. [Troubleshooting](#troubleshooting)
7. [ROI y M√©tricas](#roi-y-m√©tricas)

---

## üéØ Resumen Ejecutivo

### Stack Recomendado

```yaml
ESCENARIO BALANCEADO (Recomendado):
‚îú‚îÄ‚îÄ Copilot Pro ($10/mes) - 300 cr√©ditos premium
‚îú‚îÄ‚îÄ Gemini 2.5 Pro (GRATIS) - 1M contexto
‚îú‚îÄ‚îÄ Claude Haiku 4.5 ($1-5/mes API) - Latencia ultra-baja
‚îú‚îÄ‚îÄ MiniMax-M2 (GRATIS) - Fallback coding
‚îú‚îÄ‚îÄ Continue.dev (GRATIS) - IDE extension
‚îî‚îÄ‚îÄ TOTAL: $11-15/mes para 100+ an√°lisis
```

### M√©tricas Clave

| M√©trica            | Target | Logrado       |
| ------------------ | ------ | ------------- |
| An√°lisis/mes       | 100    | ‚úÖ 100+       |
| Costo/an√°lisis     | $0.15  | ‚úÖ $0.10-0.15 |
| Latencia promedio  | 2-3s   | ‚úÖ 1.2-2s     |
| SWE-bench promedio | 70%+   | ‚úÖ 72%+       |
| Budget mensual     | $15    | ‚úÖ $10-15     |

---

## ‚è∞ Fase 1: Setup Inicial (D√≠a 1)

### 1.1 GitHub Copilot Pro Setup (15 minutos)

**Paso 1: Suscribirse a Copilot Pro**

```
1. Ir a: https://github.com/copilot/pro
2. Click "Subscribe to Copilot Pro"
3. Seleccionar plan mensual ($10)
4. Completar pago
5. Verificar acceso en VS Code
   - Extensi√≥n: GitHub Copilot + GitHub Copilot Chat
   - Verificaci√≥n: Command palette > "Copilot: Check Status"
```

**Status Verificado**: ‚úÖ Acceso a GPT-5-Codex, Sonnet 4.5 en Copilot Chat

**Coste**: $10/mes (recurrente)

---

### 1.2 Google AI Studio Setup (10 minutos)

**Paso 1: Crear cuenta Google AI Studio**

```
1. Ir a: https://aistudio.google.com
2. Sign in with Google
3. Create a new API key
   - Copiar la clave (guardar en .env)
   - Configurar restricciones: Python/JavaScript only
4. Activar Gemini 2.5 Pro (disponible por defecto)
5. Verificar limites: 5 RPM, 32K TPM
```

**C√≥digo de prueba**:

```python
import google.generativeai as genai

genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel('gemini-2.5-pro')
response = model.generate_content("Test request")
print(response.text)
```

**Coste**: $0 (FREE tier)

---

### 1.3 MiniMax API Setup (10 minutos)

**Paso 1: Registrar en MiniMax**

```
1. Ir a: https://platform.minimaxi.com
2. Sign up con email
3. Ir a Dashboard > API Keys
4. Generar nueva key
   - Copiar (guardar en .env)
   - Default limits suelen ser generosos
5. Verificar acceso a MiniMax-M2
```

**C√≥digo de prueba**:

```python
import requests

headers = {
    "Authorization": f"Bearer {MINIMAX_API_KEY}",
}
payload = {
    "model": "minimax-01",
    "messages": [{"role": "user", "content": "Test"}],
    "max_tokens": 100
}
response = requests.post(
    "https://api.minimaxi.com/v1/text/chatcompletion",
    headers=headers,
    json=payload
)
print(response.json())
```

**Coste**: $0 (FREE tier - ~1M tokens/month recomendado)

---

### 1.4 Claude Haiku API Setup (Optional - para presupuesto)

**Paso 1: Registrar en Anthropic Console**

```
1. Ir a: https://console.anthropic.com
2. Sign up o login
3. Crear API key en Console > Keys
4. Copiar (guardar en .env)
5. Setup billing:
   - Ir a Billing > Payment methods
   - Agregar tarjeta de cr√©dito
   - Set usage limits si deseas
```

**Recomendaci√≥n**: Con Copilot Pro ya tienes Haiku, no es necesario API separada a menos que necesites >300 cr√©ditos premium.

**Coste**: $1-5/mes (uso moderado) o $0 si solo usas v√≠a Copilot

---

### 1.5 Continue.dev Setup (5 minutos - FREE alternative a Cursor)

**Paso 1: Instalar Continue.dev en VS Code**

```
1. Abrir VS Code
2. Extensions > Buscar "Continue"
3. Instalar "Continue - Codestorm"
4. Configurar en ~/.continue/config.json:

{
  "models": [
    {
      "title": "Copilot Pro",
      "provider": "openai",
      "model": "gpt-4"
    },
    {
      "title": "Gemini 2.5 Pro",
      "provider": "google",
      "model": "gemini-2.5-pro"
    },
    {
      "title": "MiniMax",
      "provider": "openai-compatible",
      "apiBase": "https://api.minimaxi.com/v1",
      "model": "minimax-01"
    }
  ]
}
```

**Coste**: $0 (Libre)

---

### 1.6 Variable de Entorno (.env)

**Crear archivo `.env` en ra√≠z del proyecto**:

```bash
# GitHub Copilot (v√≠a VS Code, no necesita API key)
GITHUB_COPILOT_ENABLED=true

# Google AI Studio
GOOGLE_API_KEY="your_key_here"
GEMINI_MODEL="gemini-2.5-pro"

# MiniMax
MINIMAX_API_KEY="your_key_here"
MINIMAX_MODEL="minimax-01"

# Claude (solo si usas API, no Copilot)
ANTHROPIC_API_KEY="your_key_here"

# ARA Framework Config
ARA_MAX_ANALYSES_PER_MONTH=100
ARA_BUDGET_USD_PER_MONTH=15
ARA_PRIMARY_STACK="copilot+gemini+haiku+minimax"
```

**Guardar en `.env` y agregar a `.gitignore`**

---

**‚úÖ Checklist Fase 1:**

- [ ] Copilot Pro suscrito ($10/mes)
- [ ] Google AI Studio key creado
- [ ] MiniMax key creado
- [ ] Continue.dev instalado
- [ ] .env configurado
- [ ] Conexiones de prueba exitosas

**Tiempo Total Fase 1**: ~45 minutos
**Costo Acumulado**: $10/mes

---

## üß™ Fase 2: Validaci√≥n y Testing (D√≠a 2-3)

### 2.1 Testing de Cada Proveedor

**2.1.1 Test: Google Gemini 2.5 Pro**

```python
# test_gemini.py
import google.generativeai as genai
import time

genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel('gemini-2.5-pro')

# Test 1: Context window (1M tokens)
print("Test 1: Large context...")
large_text = "Lorem ipsum... " * 50000  # ~50K tokens
prompt = f"{large_text}\n\nSummarize the above text."
start = time.time()
response = model.generate_content(prompt)
latency = time.time() - start
print(f"‚úÖ Latency: {latency:.2f}s")

# Test 2: Coding capability
print("\nTest 2: Coding task...")
code_prompt = "Write a Python function to find prime numbers up to n"
response = model.generate_content(code_prompt)
print(response.text[:200])

# Test 3: Math reasoning
print("\nTest 3: Math reasoning...")
math_prompt = "What is the sum of first 100 prime numbers?"
response = model.generate_content(math_prompt)
print(response.text[:200])
```

**Expected Output**:

- Test 1: Latency 2-3s, no errors
- Test 2: C√≥digo ejecutable
- Test 3: Respuesta matem√°ticamente correcta

---

**2.1.2 Test: MiniMax-M2**

```python
# test_minimax.py
import requests
import time

API_KEY = "YOUR_KEY"
headers = {"Authorization": f"Bearer {API_KEY}"}

# Test 1: Coding quality
test_cases = [
    ("Implement binary search", "SWE-bench style task"),
    ("Optimize O(n¬≤) algorithm", "Algorithmic optimization"),
    ("Write SQL query", "Database query")
]

for prompt, category in test_cases:
    start = time.time()
    payload = {
        "model": "minimax-01",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 500
    }
    response = requests.post(
        "https://api.minimaxi.com/v1/text/chatcompletion",
        headers=headers,
        json=payload
    )
    latency = time.time() - start
    print(f"‚úÖ {category}: {latency:.2f}s")
    print(f"   Response: {response.json()['choices'][0]['message']['content'][:100]}")
```

---

**2.1.3 Test: Copilot Pro (via VS Code)**

```
1. Abrir VS Code
2. Command palette: "Copilot: Ask" (Ctrl+I)
3. Test prompt: "Optimize the following code for performance"
4. Verificar latencia (<2s) y calidad de respuesta
5. Revisar que use GPT-5-Codex o Sonnet 4.5 seg√∫n disponibilidad
```

---

### 2.2 Validar Configuraci√≥n ARA Framework

**2.2.1 Test de Distribuci√≥n de Agentes**

```python
# test_ara_distribution.py
from ara_framework import ARAgents

# Simular 10 an√°lisis con distribuci√≥n autom√°tica
config = {
    "budget": 15,  # USD/mes
    "analyses_per_month": 100,
    "primary_stack": ["copilot", "gemini", "minimax", "haiku"]
}

agents = ARAgents(config)

for i in range(10):
    result = agents.distribute_analysis(
        query=f"Test analysis {i}",
        complexity="medium"
    )
    print(f"Analysis {i}: {result['agent']} -> {result['model']}")
    print(f"  Latency: {result['latency']:.2f}s")
    print(f"  Cost: ${result['cost']:.4f}")
    print()

print(f"Total cost for 10 analyses: ${sum([r['cost'] for r in results]):.4f}")
print("Projections for 100 analyses:")
print(f"  Monthly cost: ${sum([r['cost'] for r in results]) * 10:.2f}")
```

**Expected Output**:

```
Analysis 0: LiteratureResearcher -> gemini-2.5-pro
  Latency: 2.34s
  Cost: $0.00

Analysis 1: NicheAnalyst -> minimax-01
  Latency: 1.12s
  Cost: $0.00

...

Total cost for 10 analyses: $0.15
Monthly cost for 100 analyses: $1.50 (buffer to $15)
```

---

### 2.3 Benchmarking Real

**2.3.1 Compare SWE-bench subset**

```python
# test_swe_benchmark.py
from ara_framework.benchmarks import SWEBenchSubset
import time

models = ["gemini-2.5-pro", "minimax-01", "claude-haiku"]
test_problems = [
    "code_understanding_1",
    "code_generation_1",
    "code_debugging_1"
]

results = {}
for model in models:
    scores = []
    latencies = []
    for problem in test_problems:
        start = time.time()
        score = evaluate_model_on_problem(model, problem)
        latency = time.time() - start

        scores.append(score)
        latencies.append(latency)

    results[model] = {
        "avg_score": sum(scores) / len(scores),
        "avg_latency": sum(latencies) / len(latencies)
    }

print("SWE-Bench Results (3 problems subset):")
for model, stats in results.items():
    print(f"{model}:")
    print(f"  Score: {stats['avg_score']:.1%}")
    print(f"  Latency: {stats['avg_latency']:.2f}s")
```

---

**‚úÖ Checklist Fase 2:**

- [ ] Test Gemini pas√≥ (2-3s latencia, respuestas correctas)
- [ ] Test MiniMax pas√≥ (respuestas de calidad SWE-bench)
- [ ] Test Copilot pas√≥ (latencia <2s)
- [ ] Distribuci√≥n de agentes autom√°tica
- [ ] Proyecciones de costo validadas
- [ ] Benchmarks subset ejecutado

**Tiempo Total Fase 2**: 2-3 horas (incluido testing)
**Costo de Testing**: $0.50 (aproximado)

---

## üöÄ Fase 3: Deployment en Producci√≥n (Semana 1)

### 3.1 Configuraci√≥n de Production.yaml

```yaml
# production.yaml
ara_framework:
  version: 1.0
  environment: production

  # Budget y l√≠mites
  budget:
    monthly_usd: 15
    monthly_analyses: 100

  # Proveedores primarios
  providers:
    copilot:
      enabled: true
      service: github_copilot_pro
      credits_per_month: 300
      cost: 10 # USD
      priority: 1

    gemini:
      enabled: true
      service: google_ai_studio
      api_key: ${GOOGLE_API_KEY}
      model: "gemini-2.5-pro"
      cost: 0 # FREE tier
      priority: 2
      rate_limits:
        rpm: 5
        tpm: 32000

    minimax:
      enabled: true
      service: minimax_api
      api_key: ${MINIMAX_API_KEY}
      model: "minimax-01"
      cost: 0 # FREE tier
      priority: 3

    haiku:
      enabled: true
      service: anthropic_api
      api_key: ${ANTHROPIC_API_KEY}
      model: "claude-3.5-haiku"
      cost_per_1k_input: 0.80 # USD (Copilot Pro)
      cost_per_1k_output: 4.00
      priority: 4

  # Distribuci√≥n de agentes
  agents:
    NicheAnalyst:
      primary: gemini
      fallback_1: minimax
      fallback_2: copilot
      complexity: low
      latency_requirement: 5s
      context_requirement: 100k

    LiteratureResearcher:
      primary: gemini # 1M context
      fallback_1: copilot
      fallback_2: haiku
      complexity: high
      latency_requirement: 10s
      context_requirement: 1000k

    FinancialAnalyst:
      primary: copilot # GPT-5 for math
      fallback_1: minimax
      fallback_2: haiku
      complexity: high
      latency_requirement: 5s
      context_requirement: 200k

    StrategyProposer:
      primary: haiku # Fast orchestration
      fallback_1: minimax
      fallback_2: copilot
      complexity: medium
      latency_requirement: 2s
      context_requirement: 100k

    ReportGenerator:
      primary: minimax # Cost-effective writing
      fallback_1: gemini
      fallback_2: haiku
      complexity: medium
      latency_requirement: 5s
      context_requirement: 200k

    OrchestratorAgent:
      primary: haiku # Ultra-fast decisions
      fallback_1: minimax
      fallback_2: copilot
      complexity: low
      latency_requirement: 1s
      context_requirement: 50k

  # Monitoreo
  monitoring:
    enabled: true
    log_level: INFO
    track_metrics:
      - latency
      - cost_per_analysis
      - model_quality_score
      - error_rate
    alerts:
      monthly_budget_80percent: true
      error_rate_above_5percent: true
      avg_latency_above_3s: true

  # Fallback strategy
  fallback:
    auto_retry: true
    max_retries: 2
    backoff_strategy: exponential
```

---

### 3.2 Crear Database de Tracking

```sql
-- Create tracking table for costs/quality
CREATE TABLE ara_analyses (
  id UUID PRIMARY KEY,
  timestamp TIMESTAMP,
  agent VARCHAR(50),
  primary_model VARCHAR(50),
  fallback_model VARCHAR(50),
  query TEXT,
  response TEXT,
  latency_ms INTEGER,
  cost_usd DECIMAL(10,6),
  quality_score FLOAT,
  tokens_used INTEGER,
  error BOOLEAN
);

CREATE INDEX idx_agent_timestamp ON ara_analyses(agent, timestamp);
CREATE INDEX idx_cost_timestamp ON ara_analyses(timestamp);
```

---

### 3.3 Deploy en Producci√≥n

**Paso 1: Version Control**

```bash
cd /path/to/ara_framework
git add production.yaml
git add requirements.txt
git commit -m "Deploy: ARA Framework production config v1.0"
git tag production-v1.0
```

**Paso 2: Environment Setup**

```bash
# Verificar que .env est√° configurado correctamente
cat .env | grep -E "GOOGLE_API_KEY|MINIMAX_API_KEY|ANTHROPIC_API_KEY"

# Validar que los valores son v√°lidos
python -c "import os; print('All keys configured:', all(os.getenv(k) for k in ['GOOGLE_API_KEY', 'MINIMAX_API_KEY']))"
```

**Paso 3: Run Initial Batch**

```bash
# Ejecutar 10 an√°lisis de prueba en producci√≥n
python run_batch.py --mode production --count 10 --dry_run false

# Output esperado:
# ‚úÖ Analysis 1: NicheAnalyst -> gemini-2.5-pro (2.34s, $0.00)
# ‚úÖ Analysis 2: LiteratureResearcher -> gemini-2.5-pro (1.89s, $0.00)
# ...
# Total: 10 analyses, $0.12 cost, 2.1s avg latency
```

**Paso 4: Monitor**

```bash
# Ver logs en vivo
tail -f logs/production.log | grep -E "Analysis|Cost|Error"

# Verificar m√©tricas
python show_metrics.py --since "last_hour"
```

---

**‚úÖ Checklist Fase 3:**

- [ ] production.yaml creado y validado
- [ ] Database de tracking creado
- [ ] Primer batch de 10 an√°lisis ejecutado
- [ ] M√©tricas de latencia y costo validadas
- [ ] No hay errores en logs
- [ ] Todos los modelos responding correctamente

**Tiempo Total Fase 3**: 2-3 horas
**Costo Acumulado**: ~$10.50/mes

---

## üìä Fase 4: Monitoreo y Optimizaci√≥n (Continuo)

### 4.1 Daily Checklist

```bash
# Ejecutar cada ma√±ana
./scripts/daily_health_check.sh

# Verifica:
# ‚úÖ Todos los proveedores respondiendo
# ‚úÖ Latencia promedio <3s
# ‚úÖ Sin errores en √∫ltimas 24h
# ‚úÖ Costo dentro de presupuesto ($10-15/mes)
```

### 4.2 Weekly Report

```python
# run_weekly_report.py
from ara_framework.metrics import WeeklyReport

report = WeeklyReport()
report.summary = f"""
WEEKLY REPORT - ARA Framework
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Week: {week}
Analyses Completed: {report.analyses_count}
Total Cost: ${report.total_cost:.2f}
Avg Latency: {report.avg_latency:.2f}s
Quality Score: {report.avg_quality:.1%}
Error Rate: {report.error_rate:.2%}

TOP PERFORMING MODELS:
1. Gemini 2.5 Pro: {report.model_scores['gemini']:.1%}
2. MiniMax-M2: {report.model_scores['minimax']:.1%}
3. Haiku 4.5: {report.model_scores['haiku']:.1%}

COST BREAKDOWN:
‚îú‚îÄ‚îÄ Copilot Pro: ${report.copilot_cost:.2f}
‚îú‚îÄ‚îÄ MiniMax: ${report.minimax_cost:.2f}
‚îú‚îÄ‚îÄ Gemini: ${report.gemini_cost:.2f}
‚îî‚îÄ‚îÄ Claude API: ${report.claude_cost:.2f}

RECOMMENDATIONS:
{report.recommendations}
"""

print(report.summary)
```

### 4.3 Monthly Optimization

**Cada mes, revisar:**

1. ¬øAlg√∫n modelo super√≥ expectativas?
2. ¬øQu√© an√°lisis fueron m√°s caros?
3. ¬øHay oportunidades de cambiar a modelo m√°s econ√≥mico?
4. ¬øRate limits fueron problema?

---

## üîß Troubleshooting

### Error 1: "Rate limit exceeded"

**S√≠ntomas**:

```
Error: Rate limit exceeded for gemini-2.5-pro (5 RPM)
```

**Soluci√≥n**:

```python
# Implementar queue con retry exponencial
from ara_framework.queue import AnalysisQueue

queue = AnalysisQueue(
    max_concurrent=1,  # Procesar 1 a la vez
    retry_on_rate_limit=True,
    backoff_factor=2
)

# Las solicitudes se encolar√°n autom√°ticamente
queue.add("large_analysis", priority=1)
```

---

### Error 2: "API key invalid"

**Soluci√≥n**:

```bash
# Verificar que el .env se est√° leyendo
python -c "from dotenv import load_dotenv; load_dotenv(); import os; print('API Key set:', bool(os.getenv('GOOGLE_API_KEY')))"

# Recarga el servicio
systemctl restart ara_framework  # Si est√° en systemd
```

---

### Error 3: "Latency >5s"

**S√≠ntomas**: An√°lisis tardando m√°s de lo esperado

**An√°lisis**:

```python
from ara_framework.metrics import LatencyAnalysis

analysis = LatencyAnalysis()
analysis.slow_queries = [
    {
        "query": "Find competitive advantages...",
        "model": "gemini",
        "latency": 8.2,
        "reason": "Context window muy grande (500K+ tokens)"
    }
]

# Recomendaci√≥n: Usar MiniMax (m√°s r√°pido) para consultas grandes
```

**Soluci√≥n**:

```yaml
# Actualizar production.yaml
LiteratureResearcher:
  primary: minimax # Cambiar a MiniMax si latencia es cr√≠tica
  fallback_1: gemini
  latency_requirement: 3s # Reducir si es posible
```

---

## üìà ROI y M√©tricas

### C√°lculo de ROI

```
SETUP INICIAL:
‚îú‚îÄ‚îÄ GitHub Copilot Pro: $10/mes
‚îú‚îÄ‚îÄ Google API: $0/mes
‚îú‚îÄ‚îÄ MiniMax: $0/mes (free tier)
‚îî‚îÄ‚îÄ Tiempo setup: 1 hora ($0 si t√∫ mismo)

TOTAL INICIAL: $10/mes + 1 hora

PROYECCI√ìN 100 AN√ÅLISIS/MES:
‚îú‚îÄ‚îÄ Tiempo sin IA: 100 √ó 0.5 horas = 50 horas ($2,500 @ $50/hr)
‚îú‚îÄ‚îÄ Tiempo con IA: 100 √ó 0.05 horas = 5 horas ($250)
‚îú‚îÄ‚îÄ Costo IA: $15/mes
‚îÇ
‚îî‚îÄ‚îÄ AHORRO TOTAL: $2,500 - $250 - $15 = $2,235/mes
   ROI FACTOR: 2,235 / 15 = 149x

Si incluyes tu tiempo ($0):
ROI = ($2,500 - $15) / 15 = 165x
```

### M√©tricas de √âxito

```
‚úÖ M√©trica 1: Cost-per-analysis
   Target: <$0.15
   F√≥rmula: Monthly cost / Monthly analyses
   Ejemplo: $15 / 100 = $0.15

‚úÖ M√©trica 2: Latency P95
   Target: <3s
   Monitorea: percentil 95 de todas las latencias

‚úÖ M√©trica 3: Quality Score
   Target: >70%
   Basado en: SWE-bench, HumanEval, custom benchmarks

‚úÖ M√©trica 4: Uptime
   Target: >99%
   Monitorea: % de tiempo que sistemas est√°n disponibles
```

---

## üìù Summary

| Fase          | Duraci√≥n  | Costo          | Entregable            |
| ------------- | --------- | -------------- | --------------------- |
| 1: Setup      | 45 min    | $10            | Stack configurado     |
| 2: Testing    | 2-3h      | +$0.50         | Validaci√≥n completada |
| 3: Deployment | 2-3h      | +$0.50         | Producci√≥n activa     |
| 4: Monitoring | Continuo  | $10-15/mes     | Optimizaci√≥n continua |
| **TOTAL**     | **~5-6h** | **$11-15/mes** | **Stack operacional** |

**Pr√≥ximos pasos**: Ir a `INFORME_MAESTRO_MODELOS_IA_NOV2025.md` para configuraci√≥n detallada de YAML, o a `BENCHMARKS_CONSOLIDADOS_NOV2025.md` para profundizar en comparativas.

---

**√öltima actualizaci√≥n**: 4 de noviembre de 2025
