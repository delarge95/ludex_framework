"""
Script de comparaci√≥n: GitHub Models (gpt-4o) vs Ollama (mistral:7b)

Ejecuta el Agent 1 (Niche Analyst) con ambos proveedores y compara:
- Tiempo de ejecuci√≥n
- Calidad de output
- Uso de herramientas
- Longitud y coherencia de respuestas

Uso:
    python test_ollama_vs_github.py
"""

import asyncio
import time
import os
from datetime import datetime

import structlog
from graphs.research_graph import niche_analyst_node, ResearchState

logger = structlog.get_logger(__name__)


async def test_provider(provider_name: str, niche: str) -> dict:
    """
    Ejecuta Agent 1 con un proveedor espec√≠fico.
    
    Args:
        provider_name: "github" o "ollama"
        niche: Topic de investigaci√≥n
        
    Returns:
        dict con m√©tricas y resultados
    """
    print(f"\n{'='*70}")
    print(f"PRUEBA CON {provider_name.upper()}")
    print(f"{'='*70}\n")
    
    # Configurar variable de entorno
    os.environ["USE_OLLAMA"] = "true" if provider_name == "ollama" else "false"
    
    # Reimportar para aplicar cambios
    import importlib
    import graphs.research_graph as rg
    importlib.reload(rg)
    
    # Estado inicial
    initial_state: ResearchState = {
        "niche": niche,
        "niche_analysis": None,
        "literature_review": None,
        "technical_architecture": None,
        "implementation_plan": None,
        "final_report": None,
        "messages": [],
        "current_agent": "niche_analyst",
        "agent_history": [],
        "start_time": datetime.now().isoformat(),
        "end_time": None,
        "errors": [],
        "warnings": [],
        "retry_count": {},
        "total_credits_used": 0.0,
        "budget_limit": 10.0,
        "budget_exceeded": False,
    }
    
    # Ejecutar y medir tiempo
    start_time = time.time()
    
    try:
        result_state = await rg.niche_analyst_node(initial_state)
        execution_time = time.time() - start_time
        
        # Analizar resultados
        analysis = result_state.get("niche_analysis", "")
        
        metrics = {
            "provider": provider_name,
            "success": True,
            "execution_time": execution_time,
            "output_length": len(analysis),
            "output_preview": analysis[:500] if analysis else "NO OUTPUT",
            "has_viability_score": "viability score" in analysis.lower() if analysis else False,
            "has_trends": "trends" in analysis.lower() if analysis else False,
            "has_keywords": "keywords" in analysis.lower() if analysis else False,
            "errors": result_state.get("errors", []),
            "warnings": result_state.get("warnings", []),
        }
        
        print(f"\n‚úÖ Ejecuci√≥n exitosa")
        print(f"‚è±Ô∏è  Tiempo: {execution_time:.2f} segundos")
        print(f"üìù Longitud output: {metrics['output_length']} caracteres")
        print(f"üéØ Viability Score presente: {metrics['has_viability_score']}")
        print(f"üìä Trends presentes: {metrics['has_trends']}")
        print(f"üîë Keywords presentes: {metrics['has_keywords']}")
        
        if metrics['errors']:
            print(f"‚ö†Ô∏è  Errores: {len(metrics['errors'])}")
            for error in metrics['errors']:
                print(f"   - {error}")
        
        print(f"\n--- Preview del Output (primeros 500 chars) ---")
        print(metrics['output_preview'])
        print(f"{'='*70}\n")
        
        return metrics
        
    except Exception as e:
        execution_time = time.time() - start_time
        
        print(f"\n‚ùå Error durante ejecuci√≥n")
        print(f"‚è±Ô∏è  Tiempo hasta error: {execution_time:.2f} segundos")
        print(f"üî¥ Error: {str(e)}")
        print(f"{'='*70}\n")
        
        return {
            "provider": provider_name,
            "success": False,
            "execution_time": execution_time,
            "error": str(e),
            "output_length": 0,
        }


async def main():
    """Ejecuta comparaci√≥n completa."""
    
    print("\n" + "="*70)
    print(" COMPARACI√ìN: GITHUB MODELS vs OLLAMA")
    print("="*70)
    print("\nüìã Configuraci√≥n:")
    print("   - Agent: Niche Analyst (Agent 1)")
    print("   - GitHub Model: gpt-4o")
    print("   - Ollama Model: mistral:7b")
    print("   - Niche: 'deep learning for drug discovery'")
    print("\n‚è≥ Nota: Cada prueba tarda ~5-8 minutos")
    print("="*70)
    
    # Niche de prueba (mismo para ambos)
    test_niche = "deep learning for drug discovery"
    
    # Ejecutar con GitHub Models
    github_metrics = await test_provider("github", test_niche)
    
    # Ejecutar con Ollama
    ollama_metrics = await test_provider("ollama", test_niche)
    
    # Comparaci√≥n final
    print("\n" + "="*70)
    print(" COMPARACI√ìN FINAL")
    print("="*70)
    
    if github_metrics.get("success") and ollama_metrics.get("success"):
        print("\n‚úÖ Ambas pruebas completadas exitosamente\n")
        
        print("‚è±Ô∏è  TIEMPO DE EJECUCI√ìN:")
        print(f"   GitHub (gpt-4o):     {github_metrics['execution_time']:.2f}s")
        print(f"   Ollama (mistral:7b): {ollama_metrics['execution_time']:.2f}s")
        speed_diff = ((ollama_metrics['execution_time'] - github_metrics['execution_time']) 
                      / github_metrics['execution_time'] * 100)
        print(f"   Diferencia:          {speed_diff:+.1f}%")
        
        print("\nüìù LONGITUD DE OUTPUT:")
        print(f"   GitHub (gpt-4o):     {github_metrics['output_length']:,} caracteres")
        print(f"   Ollama (mistral:7b): {ollama_metrics['output_length']:,} caracteres")
        length_diff = ((ollama_metrics['output_length'] - github_metrics['output_length']) 
                       / github_metrics['output_length'] * 100)
        print(f"   Diferencia:          {length_diff:+.1f}%")
        
        print("\nüéØ COMPONENTES REQUERIDOS:")
        print(f"   Viability Score:")
        print(f"      GitHub:  {'‚úÖ' if github_metrics['has_viability_score'] else '‚ùå'}")
        print(f"      Ollama:  {'‚úÖ' if ollama_metrics['has_viability_score'] else '‚ùå'}")
        print(f"   Trends:")
        print(f"      GitHub:  {'‚úÖ' if github_metrics['has_trends'] else '‚ùå'}")
        print(f"      Ollama:  {'‚úÖ' if ollama_metrics['has_trends'] else '‚ùå'}")
        print(f"   Keywords:")
        print(f"      GitHub:  {'‚úÖ' if github_metrics['has_keywords'] else '‚ùå'}")
        print(f"      Ollama:  {'‚úÖ' if ollama_metrics['has_keywords'] else '‚ùå'}")
        
        print("\n" + "="*70)
        print(" RECOMENDACI√ìN")
        print("="*70)
        
        # An√°lisis de calidad
        ollama_quality_score = sum([
            ollama_metrics['has_viability_score'],
            ollama_metrics['has_trends'],
            ollama_metrics['has_keywords'],
            ollama_metrics['output_length'] > 1000,
        ])
        
        if ollama_quality_score >= 3:
            print("\n‚úÖ OLLAMA APTO PARA DESARROLLO")
            print("   - Tool calling funciona correctamente")
            print("   - Output cumple requisitos b√°sicos")
            print("   - Recomendado para iteraci√≥n r√°pida")
            print("\nüí° Estrategia sugerida:")
            print("   ‚Ä¢ Desarrollo/pruebas: Ollama (ilimitado)")
            print("   ‚Ä¢ Validaci√≥n final: GitHub Models (mayor calidad)")
        elif ollama_quality_score >= 2:
            print("\n‚ö†Ô∏è  OLLAMA PARCIALMENTE APTO")
            print("   - Tool calling funciona")
            print("   - Output incompleto o inferior")
            print("\nüí° Estrategia sugerida:")
            print("   ‚Ä¢ Agentes simples: Ollama")
            print("   ‚Ä¢ Agentes complejos: GitHub Models")
        else:
            print("\n‚ùå OLLAMA NO RECOMENDADO")
            print("   - Calidad muy inferior a GitHub Models")
            print("\nüí° Estrategia sugerida:")
            print("   ‚Ä¢ Usar solo GitHub Models")
            print("   ‚Ä¢ Optimizar uso (caching, rate limiting)")
        
    else:
        print("\n‚ùå Una o ambas pruebas fallaron")
        if not github_metrics.get("success"):
            print(f"   GitHub error: {github_metrics.get('error')}")
        if not ollama_metrics.get("success"):
            print(f"   Ollama error: {ollama_metrics.get('error')}")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    asyncio.run(main())
